{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# TechStore - ETL Pipeline\n",
    "## Business Intelligence Project\n",
    "### Data Extraction, Transformation, and Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "config",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "TECHSTORE - ETL PIPELINE\n",
      "======================================================================\n",
      "Project Root: c:\\Users\\windows\\TechStore\\src\n",
      "Execution: 2026-01-27 22:30:04\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "PROJECT_ROOT = Path.cwd()\n",
    "SCRIPTS_DIR = PROJECT_ROOT / 'scripts'\n",
    "DATA_DIR = PROJECT_ROOT / 'Data'\n",
    "EXTRACTED_DIR = DATA_DIR / 'extracted'\n",
    "TRANSFORMED_DIR = DATA_DIR / 'transformed'\n",
    "DATABASE_DIR = PROJECT_ROOT / 'database'\n",
    "\n",
    "for directory in [EXTRACTED_DIR, TRANSFORMED_DIR, DATABASE_DIR]:\n",
    "    directory.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "sys.path.insert(0, str(SCRIPTS_DIR))\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"TECHSTORE - ETL PIPELINE\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Project Root: {PROJECT_ROOT}\")\n",
    "print(f\"Execution: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extraction-header",
   "metadata": {},
   "source": [
    "## 1. Data Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "mysql-extraction",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "MYSQL EXTRACTION\n",
      "======================================================================\n",
      "2026-01-27 22:30:05,373 - INFO - Connexion réussie à techstore_erp\n",
      "MySQL connection established\n",
      "\n",
      "2026-01-27 22:30:05,376 - INFO - \n",
      "============================================================\n",
      "2026-01-27 22:30:05,378 - INFO -  DÉBUT DE L'EXTRACTION COMPLÈTE\n",
      "2026-01-27 22:30:05,379 - INFO - ============================================================\n",
      "\n",
      "2026-01-27 22:30:05,713 - INFO - \n",
      "Informations sur table_sales:\n",
      "2026-01-27 22:30:05,715 - INFO -    Nombre de lignes: 25000\n",
      "2026-01-27 22:30:05,716 - INFO -    Colonnes: Trans_ID, Date, Store_ID, Product_ID, Customer_ID, Quantity, Total_Revenue\n",
      "2026-01-27 22:30:05,719 - INFO -  Extraction de la table: table_sales\n",
      "2026-01-27 22:30:08,307 - INFO - 25000 lignes extraites de table_sales\n",
      "2026-01-27 22:30:08,311 - INFO - Fichier sauvegardé: data/extracted/sales.csv\n",
      "2026-01-27 22:30:08,312 - INFO - \n",
      "------------------------------------------------------------\n",
      "\n",
      "2026-01-27 22:30:08,643 - INFO - \n",
      "Informations sur table_products:\n",
      "2026-01-27 22:30:08,645 - INFO -    Nombre de lignes: 38\n",
      "2026-01-27 22:30:08,647 - INFO -    Colonnes: Product_ID, Product_Name, SubCat_ID, Unit_Price, Unit_Cost\n",
      "2026-01-27 22:30:08,649 - INFO -  Extraction de la table: table_products\n",
      "2026-01-27 22:30:08,824 - INFO - 38 lignes extraites de table_products\n",
      "2026-01-27 22:30:08,827 - INFO - Fichier sauvegardé: data/extracted/products.csv\n",
      "2026-01-27 22:30:08,831 - INFO - \n",
      "------------------------------------------------------------\n",
      "\n",
      "2026-01-27 22:30:09,163 - INFO - \n",
      "Informations sur table_reviews:\n",
      "2026-01-27 22:30:09,164 - INFO -    Nombre de lignes: 3000\n",
      "2026-01-27 22:30:09,166 - INFO -    Colonnes: Review_ID, Product_ID, Customer_ID, Rating, Review_Text\n",
      "2026-01-27 22:30:09,168 - INFO -  Extraction de la table: table_reviews\n",
      "2026-01-27 22:30:09,547 - INFO - 3000 lignes extraites de table_reviews\n",
      "2026-01-27 22:30:09,557 - INFO - Fichier sauvegardé: data/extracted/reviews.csv\n",
      "2026-01-27 22:30:09,563 - INFO - \n",
      "------------------------------------------------------------\n",
      "\n",
      "2026-01-27 22:30:09,950 - INFO - \n",
      "Informations sur table_customers:\n",
      "2026-01-27 22:30:09,953 - INFO -    Nombre de lignes: 1200\n",
      "2026-01-27 22:30:09,956 - INFO -    Colonnes: Customer_ID, Full_Name, City_ID\n",
      "2026-01-27 22:30:09,960 - INFO -  Extraction de la table: table_customers\n",
      "2026-01-27 22:30:10,164 - INFO - 1200 lignes extraites de table_customers\n",
      "2026-01-27 22:30:10,166 - INFO - Fichier sauvegardé: data/extracted/customers.csv\n",
      "2026-01-27 22:30:10,168 - INFO - \n",
      "------------------------------------------------------------\n",
      "\n",
      "2026-01-27 22:30:10,519 - INFO - \n",
      "Informations sur table_stores:\n",
      "2026-01-27 22:30:10,521 - INFO -    Nombre de lignes: 12\n",
      "2026-01-27 22:30:10,526 - INFO -    Colonnes: Store_ID, Store_Name, City_ID\n",
      "2026-01-27 22:30:10,528 - INFO -  Extraction de la table: table_stores\n",
      "2026-01-27 22:30:10,722 - INFO - 12 lignes extraites de table_stores\n",
      "2026-01-27 22:30:10,726 - INFO - Fichier sauvegardé: data/extracted/stores.csv\n",
      "2026-01-27 22:30:10,728 - INFO - \n",
      "------------------------------------------------------------\n",
      "\n",
      "2026-01-27 22:30:11,069 - INFO - \n",
      "Informations sur table_cities:\n",
      "2026-01-27 22:30:11,073 - INFO -    Nombre de lignes: 12\n",
      "2026-01-27 22:30:11,076 - INFO -    Colonnes: City_ID, City_Name, Region\n",
      "2026-01-27 22:30:11,079 - INFO -  Extraction de la table: table_cities\n",
      "2026-01-27 22:30:11,263 - INFO - 12 lignes extraites de table_cities\n",
      "2026-01-27 22:30:11,266 - INFO - Fichier sauvegardé: data/extracted/cities.csv\n",
      "2026-01-27 22:30:11,267 - INFO - \n",
      "------------------------------------------------------------\n",
      "\n",
      "2026-01-27 22:30:11,615 - INFO - \n",
      "Informations sur table_categories:\n",
      "2026-01-27 22:30:11,616 - INFO -    Nombre de lignes: 5\n",
      "2026-01-27 22:30:11,618 - INFO -    Colonnes: Category_ID, Category_Name\n",
      "2026-01-27 22:30:11,622 - INFO -  Extraction de la table: table_categories\n",
      "2026-01-27 22:30:11,781 - INFO - 5 lignes extraites de table_categories\n",
      "2026-01-27 22:30:11,783 - INFO - Fichier sauvegardé: data/extracted/categories.csv\n",
      "2026-01-27 22:30:11,785 - INFO - \n",
      "------------------------------------------------------------\n",
      "\n",
      "2026-01-27 22:30:12,125 - INFO - \n",
      "Informations sur table_subcategories:\n",
      "2026-01-27 22:30:12,128 - INFO -    Nombre de lignes: 15\n",
      "2026-01-27 22:30:12,131 - INFO -    Colonnes: SubCat_ID, SubCat_Name, Category_ID\n",
      "2026-01-27 22:30:12,133 - INFO -  Extraction de la table: table_subcategories\n",
      "2026-01-27 22:30:12,314 - INFO - 15 lignes extraites de table_subcategories\n",
      "2026-01-27 22:30:12,316 - INFO - Fichier sauvegardé: data/extracted/subcategories.csv\n",
      "2026-01-27 22:30:12,318 - INFO - \n",
      "------------------------------------------------------------\n",
      "\n",
      "2026-01-27 22:30:12,320 - INFO - \n",
      "============================================================\n",
      "2026-01-27 22:30:12,323 - INFO -  RÉSUMÉ DE L'EXTRACTION\n",
      "2026-01-27 22:30:12,326 - INFO - ============================================================\n",
      "\n",
      "2026-01-27 22:30:12,332 - INFO - \n",
      "              Table  Lignes  Colonnes Statut\n",
      "        table_sales   25000         7 Succès\n",
      "     table_products      38         5 Succès\n",
      "      table_reviews    3000         5 Succès\n",
      "    table_customers    1200         3 Succès\n",
      "       table_stores      12         3 Succès\n",
      "       table_cities      12         3 Succès\n",
      "   table_categories       5         2 Succès\n",
      "table_subcategories      15         3 Succès\n",
      "\n",
      "2026-01-27 22:30:12,421 - INFO - Connexion fermée\n",
      "\n",
      "8 tables extracted successfully\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MYSQL EXTRACTION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "MYSQL_CONFIG = {\n",
    "    'host': 'boughida.com',\n",
    "    'database': 'techstore_erp',\n",
    "    'user': 'student_user_4ing',\n",
    "    'password': 'bi_guelma_2025'\n",
    "}\n",
    "\n",
    "try:\n",
    "    from extract_mysql import MySQLExtractor\n",
    "    \n",
    "    extractor = MySQLExtractor(**MYSQL_CONFIG)\n",
    "    \n",
    "    if extractor.connect():\n",
    "        print(\"MySQL connection established\\n\")\n",
    "        extraction_summary = extractor.extract_all_tables()\n",
    "        extractor.close()\n",
    "        print(f\"\\n{len(extraction_summary)} tables extracted successfully\")\n",
    "    else:\n",
    "        print(\"MySQL connection failed\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error in MySQL extraction: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "web-scraping",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "WEB SCRAPING - COMPETITOR PRICES\n",
      "======================================================================\n",
      "2026-01-27 22:30:12,455 - INFO - ============================================================\n",
      "2026-01-27 22:30:12,458 - INFO - Starting web scraping process\n",
      "2026-01-27 22:30:12,461 - INFO - ============================================================\n",
      "2026-01-27 22:30:12,462 - INFO - Fetching page: https://boughida.com/competitor/\n",
      "2026-01-27 22:30:13,868 - INFO - Page successfully retrieved\n",
      "2026-01-27 22:30:13,880 - INFO - Found additional pages: 2\n",
      "2026-01-27 22:30:13,883 - INFO - \n",
      "Page 1/3: https://boughida.com/competitor/\n",
      "2026-01-27 22:30:13,885 - INFO - ------------------------------------------------------------\n",
      "2026-01-27 22:30:13,888 - INFO - Fetching page: https://boughida.com/competitor/\n",
      "2026-01-27 22:30:15,144 - INFO - Page successfully retrieved\n",
      "2026-01-27 22:30:15,152 - INFO - Found 13 potential product containers\n",
      "2026-01-27 22:30:15,158 - INFO - Extracted: Samsung S23 Ultra: 174800.0 DZD\n",
      "2026-01-27 22:30:15,161 - INFO - Extracted: HP LaserJet Pro: 46000.0 DZD\n",
      "2026-01-27 22:30:15,164 - INFO - Extracted: Dell 24 Monitor: 24200.0 DZD\n",
      "2026-01-27 22:30:15,167 - INFO - Extracted: Canon i-Sensys: 39000.0 DZD\n",
      "2026-01-27 22:30:15,169 - INFO - Extracted: Epson EcoTank: 27700.0 DZD\n",
      "2026-01-27 22:30:15,175 - INFO - Extracted: Nikon D3500: 69200.0 DZD\n",
      "2026-01-27 22:30:15,180 - INFO - Extracted: Xiaomi Redmi Note 12: 36800.0 DZD\n",
      "2026-01-27 22:30:15,184 - INFO - Extracted: HP Pavilion Desktop: 83000.0 DZD\n",
      "2026-01-27 22:30:15,190 - INFO - Extracted: Redmi Buds 4: 5300.0 DZD\n",
      "2026-01-27 22:30:15,194 - INFO - Extracted: DJI Mini 3: 126900.0 DZD\n",
      "2026-01-27 22:30:15,197 - INFO - Extracted: Sony SRS-XB13: 11100.0 DZD\n",
      "2026-01-27 22:30:15,206 - INFO - Extracted: ASUS ROG STRIX: 290600.0 DZD\n",
      "2026-01-27 22:30:15,210 - INFO - Extracted: LG UltraGear: 41700.0 DZD\n",
      "2026-01-27 22:30:17,214 - INFO - \n",
      "Page 2/3: https://boughida.com/competitor/competitor_page_2.html\n",
      "2026-01-27 22:30:17,216 - INFO - ------------------------------------------------------------\n",
      "2026-01-27 22:30:17,218 - INFO - Fetching page: https://boughida.com/competitor/competitor_page_2.html\n",
      "2026-01-27 22:30:18,444 - INFO - Page successfully retrieved\n",
      "2026-01-27 22:30:18,450 - INFO - Found 13 potential product containers\n",
      "2026-01-27 22:30:18,453 - INFO - Extracted: IPHONE CABLE: 3200.0 DZD\n",
      "2026-01-27 22:30:18,456 - INFO - Extracted: Lenovo ThinkPad: 94600.0 DZD\n",
      "2026-01-27 22:30:18,458 - INFO - Extracted: AirPods Pro: 47400.0 DZD\n",
      "2026-01-27 22:30:18,460 - INFO - Extracted: Phone Case Silicon: 1800.0 DZD\n",
      "2026-01-27 22:30:18,462 - INFO - Extracted: Sony WH-1000XM5: 65400.0 DZD\n",
      "2026-01-27 22:30:18,463 - INFO - Extracted: iPhone 13: 136900.0 DZD\n",
      "2026-01-27 22:30:18,466 - INFO - Extracted: Canon 445 Black: 2600.0 DZD\n",
      "2026-01-27 22:30:18,471 - INFO - Extracted: DJI Mavic 3: 350400.0 DZD\n",
      "2026-01-27 22:30:18,473 - INFO - Extracted: JBL Flip 6: 23500.0 DZD\n",
      "2026-01-27 22:30:18,477 - INFO - Extracted: HP VICTUS 15: 119700.0 DZD\n",
      "2026-01-27 22:30:18,481 - INFO - Extracted: HP 650 Black: 2900.0 DZD\n",
      "2026-01-27 22:30:18,486 - INFO - Extracted: Oppo Reno 8: 60200.0 DZD\n",
      "2026-01-27 22:30:18,492 - INFO - Extracted: SAMSUNG CHARGER 25W: 4200.0 DZD\n",
      "2026-01-27 22:30:20,496 - INFO - \n",
      "Page 3/3: https://boughida.com/competitor/competitor_page_3.html\n",
      "2026-01-27 22:30:20,497 - INFO - ------------------------------------------------------------\n",
      "2026-01-27 22:30:20,499 - INFO - Fetching page: https://boughida.com/competitor/competitor_page_3.html\n",
      "2026-01-27 22:30:22,186 - INFO - Page successfully retrieved\n",
      "2026-01-27 22:30:22,196 - INFO - Found 12 potential product containers\n",
      "2026-01-27 22:30:22,200 - INFO - Extracted: MacBook Air M2: 190500.0 DZD\n",
      "2026-01-27 22:30:22,209 - INFO - Extracted: Canon Pixma: 11600.0 DZD\n",
      "2026-01-27 22:30:22,213 - INFO - Extracted: Canon EOS 4000D: 60200.0 DZD\n",
      "2026-01-27 22:30:22,216 - INFO - Extracted: JBL Tune 510BT: 11200.0 DZD\n",
      "2026-01-27 22:30:22,225 - INFO - Extracted: Screen Protector: 930.0 DZD\n",
      "2026-01-27 22:30:22,229 - INFO - Extracted: Custom Gaming PC: 300000.0 DZD\n",
      "2026-01-27 22:30:22,233 - INFO - Extracted: Samsung Odyssey G5: 46700.0 DZD\n",
      "2026-01-27 22:30:22,241 - INFO - Extracted: Realme GT3: 71700.0 DZD\n",
      "2026-01-27 22:30:22,245 - INFO - Extracted: iPhone 14 Pro: 212700.0 DZD\n",
      "2026-01-27 22:30:22,246 - INFO - Extracted: IPHONE 15 PRO MAX: 281500.0 DZD\n",
      "2026-01-27 22:30:22,249 - INFO - Extracted: Dell XPS 13 5G: 242700.0 DZD\n",
      "2026-01-27 22:30:22,253 - INFO - Extracted: Dell OptiPlex: 64400.0 DZD\n",
      "2026-01-27 22:30:22,259 - INFO - ============================================================\n",
      "2026-01-27 22:30:22,262 - INFO - Scraping completed: 38 total products\n",
      "2026-01-27 22:30:22,266 - INFO - ============================================================\n",
      "2026-01-27 22:30:22,326 - INFO - \n",
      "Data saved to: data/extracted/competitor_prices.csv\n",
      "2026-01-27 22:30:22,330 - INFO - Statistics:\n",
      "2026-01-27 22:30:22,340 - INFO -   Total products: 38\n",
      "2026-01-27 22:30:22,345 - INFO -   Average price: 89032.37 DZD\n",
      "2026-01-27 22:30:22,349 - INFO -   Price range: 930 - 350400 DZD\n",
      "\n",
      "38 competitor prices extracted\n",
      "File: c:\\Users\\windows\\TechStore\\src\\Data\\extracted\\competitor_prices.csv\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"WEB SCRAPING - COMPETITOR PRICES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "try:\n",
    "    from scrape_competitors import scrape_with_fallback\n",
    "    \n",
    "    df_competitor = scrape_with_fallback()\n",
    "    \n",
    "    if df_competitor is not None and len(df_competitor) > 0:\n",
    "        print(f\"\\n{len(df_competitor)} competitor prices extracted\")\n",
    "        print(f\"File: {EXTRACTED_DIR / 'competitor_prices.csv'}\")\n",
    "    else:\n",
    "        print(\"No competitor data retrieved\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error in web scraping: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ocr-extraction",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "OCR - LEGACY INVOICES (OPTIONAL)\n",
      "======================================================================\n",
      "5 invoices detected\n",
      "\n",
      "2026-01-27 22:30:22,483 - INFO - Tesseract found in PATH: C:\\Program Files\\Tesseract-OCR\\tesseract.EXE\n",
      "2026-01-27 22:30:22,494 - INFO - ======================================================================\n",
      "2026-01-27 22:30:22,504 - INFO - STARTING OCR INVOICE PROCESSING\n",
      "2026-01-27 22:30:22,508 - INFO - ======================================================================\n",
      "2026-01-27 22:30:22,512 - INFO - Found 5 invoice images to process\n",
      "\n",
      "2026-01-27 22:30:22,515 - INFO - \n",
      "======================================================================\n",
      "2026-01-27 22:30:22,522 - INFO - PROCESSING [1/5]: order_001.jpg\n",
      "2026-01-27 22:30:22,526 - INFO - ======================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-27 22:30:26,178 - INFO - Extracted from order_001.jpg:\n",
      "2026-01-27 22:30:26,182 - INFO -   [OK] Date: 2022-09-22\n",
      "2026-01-27 22:30:26,186 - INFO -   [OK] Order_Reference: ORD-5073\n",
      "2026-01-27 22:30:26,190 - INFO -   [OK] Customer_ID: C1001\n",
      "2026-01-27 22:30:26,193 - INFO -   [OK] Customer_Name: Sami Oukil\n",
      "2026-01-27 22:30:26,233 - INFO -   [OK] Product_Name: HP Victus 15\n",
      "2026-01-27 22:30:26,238 - INFO -   [OK] Quantity: 2\n",
      "2026-01-27 22:30:26,240 - INFO -   [OK] Unit_Price: 125000.0\n",
      "2026-01-27 22:30:26,243 - INFO -   [OK] Total_Revenue: 250000.0\n",
      "2026-01-27 22:30:26,245 - INFO - \n",
      "======================================================================\n",
      "2026-01-27 22:30:26,248 - INFO - PROCESSING [2/5]: order_002.jpg\n",
      "2026-01-27 22:30:26,249 - INFO - ======================================================================\n",
      "2026-01-27 22:30:29,616 - INFO - Extracted from order_002.jpg:\n",
      "2026-01-27 22:30:29,622 - INFO -   [OK] Date: 2022-02-20\n",
      "2026-01-27 22:30:29,624 - INFO -   [--] Order_Reference: None\n",
      "2026-01-27 22:30:29,625 - INFO -   [OK] Customer_ID: C1003\n",
      "2026-01-27 22:30:29,629 - INFO -   [OK] Customer_Name: Meriem Bouzid\n",
      "2026-01-27 22:30:29,630 - INFO -   [OK] Product_Name: MacBook Alr M2\n",
      "2026-01-27 22:30:29,633 - INFO -   [OK] Quantity: 1\n",
      "2026-01-27 22:30:29,640 - INFO -   [OK] Unit_Price: 195000.0\n",
      "2026-01-27 22:30:29,641 - INFO -   [OK] Total_Revenue: 195000.0\n",
      "2026-01-27 22:30:29,644 - INFO - \n",
      "======================================================================\n",
      "2026-01-27 22:30:29,656 - INFO - PROCESSING [3/5]: order_003.jpg\n",
      "2026-01-27 22:30:29,661 - INFO - ======================================================================\n",
      "2026-01-27 22:30:32,799 - INFO - Extracted from order_003.jpg:\n",
      "2026-01-27 22:30:32,806 - INFO -   [OK] Date: 2022-06-01\n",
      "2026-01-27 22:30:32,809 - INFO -   [OK] Order_Reference: ORD-8447\n",
      "2026-01-27 22:30:32,812 - INFO -   [OK] Customer_ID: C1004\n",
      "2026-01-27 22:30:32,815 - INFO -   [OK] Customer_Name: Amine Rahmani\n",
      "2026-01-27 22:30:32,820 - INFO -   [OK] Product_Name: Samsung S23 Ultra\n",
      "2026-01-27 22:30:32,824 - INFO -   [OK] Quantity: 3\n",
      "2026-01-27 22:30:32,827 - INFO -   [OK] Unit_Price: 185000.0\n",
      "2026-01-27 22:30:32,830 - INFO -   [OK] Total_Revenue: 555000.0\n",
      "2026-01-27 22:30:32,833 - INFO - \n",
      "======================================================================\n",
      "2026-01-27 22:30:32,840 - INFO - PROCESSING [4/5]: order_004.jpg\n",
      "2026-01-27 22:30:32,842 - INFO - ======================================================================\n",
      "2026-01-27 22:30:36,771 - INFO - Extracted from order_004.jpg:\n",
      "2026-01-27 22:30:36,773 - INFO -   [OK] Date: 2022-10-27\n",
      "2026-01-27 22:30:36,775 - INFO -   [OK] Order_Reference: ORD-7940\n",
      "2026-01-27 22:30:36,777 - INFO -   [OK] Customer_ID: C1025\n",
      "2026-01-27 22:30:36,781 - INFO -   [OK] Customer_Name: Houda Mekki\n",
      "2026-01-27 22:30:36,783 - INFO -   [OK] Product_Name: iPhone 14 Pro\n",
      "2026-01-27 22:30:36,787 - INFO -   [OK] Quantity: 3\n",
      "2026-01-27 22:30:36,791 - INFO -   [OK] Unit_Price: 230000.0\n",
      "2026-01-27 22:30:36,793 - INFO -   [OK] Total_Revenue: 690000.0\n",
      "2026-01-27 22:30:36,795 - INFO - \n",
      "======================================================================\n",
      "2026-01-27 22:30:36,802 - INFO - PROCESSING [5/5]: order_005.jpg\n",
      "2026-01-27 22:30:36,804 - INFO - ======================================================================\n",
      "2026-01-27 22:30:39,062 - INFO - Extracted from order_005.jpg:\n",
      "2026-01-27 22:30:39,064 - INFO -   [OK] Date: 2022-02-27\n",
      "2026-01-27 22:30:39,066 - INFO -   [OK] Order_Reference: ORD-3931\n",
      "2026-01-27 22:30:39,069 - INFO -   [OK] Customer_ID: C1002\n",
      "2026-01-27 22:30:39,073 - INFO -   [OK] Customer_Name: Sofiane Khelifa\n",
      "2026-01-27 22:30:39,075 - INFO -   [OK] Product_Name: Dell XPS 13\n",
      "2026-01-27 22:30:39,076 - INFO -   [OK] Quantity: 2\n",
      "2026-01-27 22:30:39,077 - INFO -   [OK] Unit_Price: 260000.0\n",
      "2026-01-27 22:30:39,079 - INFO -   [OK] Total_Revenue: 520000.0\n",
      "2026-01-27 22:30:39,086 - INFO - \n",
      "======================================================================\n",
      "2026-01-27 22:30:39,088 - INFO - OCR PROCESSING COMPLETED\n",
      "2026-01-27 22:30:39,091 - INFO - Successfully processed: 5/5 invoices\n",
      "2026-01-27 22:30:39,092 - INFO - ======================================================================\n",
      "2026-01-27 22:30:39,098 - INFO - \n",
      "======================================================================\n",
      "2026-01-27 22:30:39,102 - INFO - DATA SUMMARY\n",
      "2026-01-27 22:30:39,104 - INFO - ======================================================================\n",
      "2026-01-27 22:30:39,106 - INFO - Total records: 5\n",
      "2026-01-27 22:30:39,108 - INFO - Total revenue: 2,210,000 DZD\n",
      "2026-01-27 22:30:39,109 - INFO - \n",
      "Data Completeness:\n",
      "2026-01-27 22:30:39,112 - INFO -   [OK] Date: 5/5 (100%)\n",
      "2026-01-27 22:30:39,114 - INFO -   [OK] Order_Reference: 4/5 (80%)\n",
      "2026-01-27 22:30:39,116 - INFO -   [OK] Customer_ID: 5/5 (100%)\n",
      "2026-01-27 22:30:39,120 - INFO -   [OK] Customer_Name: 5/5 (100%)\n",
      "2026-01-27 22:30:39,123 - INFO -   [OK] Product_Name: 5/5 (100%)\n",
      "2026-01-27 22:30:39,125 - INFO -   [OK] Quantity: 5/5 (100%)\n",
      "2026-01-27 22:30:39,126 - INFO -   [OK] Unit_Price: 5/5 (100%)\n",
      "2026-01-27 22:30:39,135 - INFO -   [OK] Total_Revenue: 5/5 (100%)\n",
      "2026-01-27 22:30:39,138 - INFO - \n",
      "[SUCCESS] Data saved to: c:\\Users\\windows\\TechStore\\src\\Data\\extracted\\legacy_sales.csv\n",
      "2026-01-27 22:30:39,140 - INFO - ======================================================================\n",
      "\n",
      "======================================================================\n",
      "EXTRACTED DATA PREVIEW\n",
      "======================================================================\n",
      "  Source_File       Date Order_Reference Customer_ID   Customer_Name      Product_Name  Quantity  Unit_Price  Total_Revenue\n",
      "order_001.jpg 2022-09-22        ORD-5073       C1001      Sami Oukil      HP Victus 15         2    125000.0       250000.0\n",
      "order_002.jpg 2022-02-20            None       C1003   Meriem Bouzid    MacBook Alr M2         1    195000.0       195000.0\n",
      "order_003.jpg 2022-06-01        ORD-8447       C1004   Amine Rahmani Samsung S23 Ultra         3    185000.0       555000.0\n",
      "order_004.jpg 2022-10-27        ORD-7940       C1025     Houda Mekki     iPhone 14 Pro         3    230000.0       690000.0\n",
      "order_005.jpg 2022-02-27        ORD-3931       C1002 Sofiane Khelifa       Dell XPS 13         2    260000.0       520000.0\n",
      "======================================================================\n",
      "\n",
      "\n",
      "5 invoices processed\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"OCR - LEGACY INVOICES (OPTIONAL)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "invoice_dir = DATA_DIR / 'legacy_invoices'\n",
    "has_invoices = invoice_dir.exists() and len(list(invoice_dir.glob('*.jpg'))) > 0\n",
    "\n",
    "if has_invoices:\n",
    "    print(f\"{len(list(invoice_dir.glob('*.jpg')))} invoices detected\\n\")\n",
    "    \n",
    "    try:\n",
    "        from extract_legacy_invoices import InvoiceOCRProcessor\n",
    "        \n",
    "        processor = InvoiceOCRProcessor(str(invoice_dir))\n",
    "        df_legacy = processor.process_and_save()\n",
    "        \n",
    "        if df_legacy is not None and len(df_legacy) > 0:\n",
    "            print(f\"\\n{len(df_legacy)} invoices processed\")\n",
    "        else:\n",
    "            print(\"No data extracted from invoices\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"OCR error: {e}\")\n",
    "else:\n",
    "    print(\"No invoices found (optional step)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "transformation-header",
   "metadata": {},
   "source": [
    "## 2. Data Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "load-extracted",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "LOADING EXTRACTED DATA\n",
      "======================================================================\n",
      "sales                 25,000 rows\n",
      "products                  38 rows\n",
      "customers              1,200 rows\n",
      "stores                    12 rows\n",
      "cities                    12 rows\n",
      "categories                 5 rows\n",
      "subcategories             15 rows\n",
      "reviews                3,000 rows\n",
      "competitor_prices         38 rows\n",
      "\n",
      "9 files loaded\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"LOADING EXTRACTED DATA\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "dataframes = {}\n",
    "\n",
    "csv_files = {\n",
    "    'sales': 'sales.csv',\n",
    "    'products': 'products.csv',\n",
    "    'customers': 'customers.csv',\n",
    "    'stores': 'stores.csv',\n",
    "    'cities': 'cities.csv',\n",
    "    'categories': 'categories.csv',\n",
    "    'subcategories': 'subcategories.csv',\n",
    "    'reviews': 'reviews.csv',\n",
    "    'competitor_prices': 'competitor_prices.csv'\n",
    "}\n",
    "\n",
    "for name, filename in csv_files.items():\n",
    "    filepath = EXTRACTED_DIR / filename\n",
    "    if filepath.exists():\n",
    "        dataframes[name] = pd.read_csv(filepath)\n",
    "        print(f\"{name:20} {len(dataframes[name]):>7,} rows\")\n",
    "    else:\n",
    "        print(f\"{name:20} File not found\")\n",
    "\n",
    "print(f\"\\n{len(dataframes)} files loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "transform-pipeline",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "RUNNING TRANSFORMATION PIPELINE\n",
      "======================================================================\n",
      "======================================================================\n",
      "ETL PIPELINE - TRANSFORMATION PHASE\n",
      "======================================================================\n",
      "\n",
      "[1/8] Loading flat files...\n",
      "✓ Flat files loaded successfully\n",
      "   Marketing: (180, 4)\n",
      "   Targets:   (432, 4)\n",
      "   Shipping:  (13, 4)\n",
      "\n",
      "[2/8] Cleaning dataframes...\n",
      "✓ Cleaning completed\n",
      "\n",
      "[3/8] Harmonizing currency...\n",
      "✓ USD → DZD conversion done\n",
      "\n",
      "[4/8] Analyzing sentiment...\n",
      "✓ Sentiment analysis done (38 products)\n",
      "\n",
      "[5/8] Creating dimension tables...\n",
      "✓ 38 competitor prices matched\n",
      "✓ dim_product created (38 products)\n",
      "   Price range: 800.00 - 360000.00 DZD\n",
      "   Categories: 5 unique\n",
      "✓ dim_store created (12 stores)\n",
      "   Regions: 4 unique\n",
      "   Average monthly target: 3,132,539.62 DZD\n",
      "   Average annual target: 37,590,475.47 DZD\n",
      "   Initial rows: 1200\n",
      "   Found 90 homonymes (same name + city) → removing\n",
      "   Examples of homonymes found:\n",
      "      'Amel Amrani' in city 11: customer_ids [85, 1148]\n",
      "      'Amel Chaouch' in city 3: customer_ids [698, 923]\n",
      "      'Amel Khelifa' in city 7: customer_ids [496, 558]\n",
      "      'Amine Djedid' in city 8: customer_ids [732, 877]\n",
      "      'Amine Mebarki' in city 8: customer_ids [148, 626]\n",
      "   After deduplication: 1110 rows\n",
      "   ✓ NO duplicates (by customer_id or name+city)\n",
      "✓ dim_customer: 1110 unique customers\n",
      "   Regions: 4\n",
      "   Cities: 12\n",
      "\n",
      "[6/8] Calculating net profit & creating fact_sales...\n",
      "\n",
      "[7/8] Creating dim_date...\n",
      "\n",
      "[8/8] Calculating marketing ROI...\n",
      "✓ Marketing ROI calculated (162 category-months)\n",
      "\n",
      "[FINAL] Saving all tables...\n",
      "✓ Saved: dim_product.csv (38 rows)\n",
      "✓ Saved: dim_store.csv (12 rows)\n",
      "✓ Saved: dim_customer.csv (1110 rows)\n",
      "✓ Saved: dim_date.csv (731 rows)\n",
      "✓ Saved: fact_sales.csv (25000 rows)\n",
      "✓ Saved: marketing_roi.csv (162 rows)\n",
      "\n",
      "✓ 6/6 tables saved to: c:\\Users\\windows\\TechStore\\src\\scripts\\../data/transformed\n",
      "\n",
      "======================================================================\n",
      "TRANSFORMATION COMPLETE - SUMMARY\n",
      "======================================================================\n",
      "dim_product:        38 rows\n",
      "dim_store:          12 rows\n",
      "dim_customer:     1110 rows\n",
      "dim_date:          731 rows\n",
      "fact_sales:      25000 rows\n",
      "marketing_roi:     162 rows\n",
      "======================================================================\n",
      "\n",
      "✅ Pipeline completed successfully!\n",
      "======================================================================\n",
      "\n",
      "Transformation completed successfully\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"RUNNING TRANSFORMATION PIPELINE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "try:\n",
    "    os.chdir(SCRIPTS_DIR)\n",
    "    import transform_data\n",
    "    transform_data.main()\n",
    "    os.chdir(PROJECT_ROOT)\n",
    "    print(\"\\nTransformation completed successfully\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Transformation error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    os.chdir(PROJECT_ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "verify-transformed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "VERIFYING TRANSFORMED FILES\n",
      "======================================================================\n",
      "Dim_Customer           1,110 rows x  5 columns\n",
      "Dim_Product               38 rows x 14 columns\n",
      "Dim_Store                 12 rows x  8 columns\n",
      "Dim_Date                 731 rows x 10 columns\n",
      "Fact_Sales            25,000 rows x 12 columns\n",
      "\n",
      "5/5 tables transformed\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"VERIFYING TRANSFORMED FILES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "transformed_files = {\n",
    "    'Dim_Customer': 'Dim_Customer.csv',\n",
    "    'Dim_Product': 'Dim_Product.csv',\n",
    "    'Dim_Store': 'Dim_Store.csv',\n",
    "    'Dim_Date': 'Dim_Date.csv',\n",
    "    'Fact_Sales': 'Fact_Sales.csv'\n",
    "}\n",
    "\n",
    "transformed_data = {}\n",
    "\n",
    "for name, filename in transformed_files.items():\n",
    "    filepath = TRANSFORMED_DIR / filename\n",
    "    if filepath.exists():\n",
    "        df = pd.read_csv(filepath)\n",
    "        transformed_data[name] = df\n",
    "        print(f\"{name:20} {len(df):>7,} rows x {len(df.columns):>2} columns\")\n",
    "    else:\n",
    "        print(f\"{name:20} File not found\")\n",
    "\n",
    "print(f\"\\n{len(transformed_data)}/5 tables transformed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "schema-preview",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "STAR SCHEMA PREVIEW\n",
      "======================================================================\n",
      "\n",
      "DIMENSIONS:\n",
      "\n",
      "Dim_Customer:\n",
      "  Rows: 1,110\n",
      "  Columns: customer_id, full_name, city_id, city_name, region...\n",
      " customer_id   full_name  city_id city_name region\n",
      "           1 Fares Mekki        9    Biskra  South\n",
      "           2 Zineb Oukil       10  Ghardaia  South\n",
      "\n",
      "Dim_Product:\n",
      "  Rows: 38\n",
      "  Columns: product_id, product_name, subcategory_id, subcategory_name, category_id...\n",
      " product_id product_name  subcategory_id subcategory_name  category_id category_name  unit_price  unit_cost  competitor_price  price_difference  price_difference_pct  avg_sentiment  avg_rating  review_count\n",
      "        100 HP Victus 15               1          Laptops            1     Computers    125000.0    87901.0          119700.0            5300.0                  4.43           0.22        3.54            97\n",
      "        101  Dell XPS 13               1          Laptops            1     Computers    260000.0   167880.0          242700.0           17300.0                  7.13           0.24        3.55            86\n",
      "\n",
      "Dim_Store:\n",
      "  Rows: 12\n",
      "  Columns: store_id, store_name, city_id, city_name, region...\n",
      " store_id             store_name  city_id city_name region  monthly_target  annual_target   manager_name\n",
      "        1 TechStore Alger Centre        1     Alger  North      6209113.48    74509361.76 Billel Rahmani\n",
      "        2   TechStore Oran Bahia        2      Oran   West      7555913.44    90670961.28 Khadidja Talbi\n",
      "\n",
      "Dim_Date:\n",
      "  Rows: 731\n",
      "  Columns: date_id, date, year, quarter, month...\n",
      " date_id       date  year  quarter  month month_name  day  day_of_week day_name  week_of_year\n",
      "20230101 2023-01-01  2023        1      1    January    1            7   Sunday            52\n",
      "20230102 2023-01-02  2023        1      1    January    2            1   Monday             1\n",
      "\n",
      "FACT TABLE:\n",
      "\n",
      "Fact_Sales:\n",
      "  Rows: 25,000\n",
      "  Columns: trans_id, date, store_id, product_id, customer_id, quantity, total_revenue, cost, gross_profit, shipping_cost, marketing_cost, net_profit\n",
      " trans_id       date  store_id  product_id  customer_id  quantity  total_revenue     cost  gross_profit  shipping_cost  marketing_cost  net_profit\n",
      "        1 2024-07-02         3         125          200         2         9000.0   6172.0        2828.0          752.0          119.63     1956.37\n",
      "        2 2023-07-01        11         136          883         1         2500.0   1606.0         894.0          376.0          100.84      417.16\n",
      "        3 2025-01-05         7         106          669         1       320000.0 198987.0      121013.0         1005.0         1949.31   118058.69\n",
      "\n",
      "Financial Summary:\n",
      "  Total Revenue: 859,661,700.00 DZD\n",
      "  Net Profit: 259,493,425.49 DZD\n",
      "  Profit Margin: 30.19%\n"
     ]
    }
   ],
   "source": [
    "if len(transformed_data) >= 5:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"STAR SCHEMA PREVIEW\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    print(\"\\nDIMENSIONS:\")\n",
    "    for dim in ['Dim_Customer', 'Dim_Product', 'Dim_Store', 'Dim_Date']:\n",
    "        if dim in transformed_data:\n",
    "            df = transformed_data[dim]\n",
    "            print(f\"\\n{dim}:\")\n",
    "            print(f\"  Rows: {len(df):,}\")\n",
    "            print(f\"  Columns: {', '.join(df.columns.tolist()[:5])}...\")\n",
    "            print(df.head(2).to_string(index=False))\n",
    "    \n",
    "    print(\"\\nFACT TABLE:\")\n",
    "    if 'Fact_Sales' in transformed_data:\n",
    "        df = transformed_data['Fact_Sales']\n",
    "        print(f\"\\nFact_Sales:\")\n",
    "        print(f\"  Rows: {len(df):,}\")\n",
    "        print(f\"  Columns: {', '.join(df.columns.tolist())}\")\n",
    "        print(df.head(3).to_string(index=False))\n",
    "        \n",
    "        print(f\"\\nFinancial Summary:\")\n",
    "        print(f\"  Total Revenue: {df['total_revenue'].sum():,.2f} DZD\")\n",
    "        print(f\"  Net Profit: {df['net_profit'].sum():,.2f} DZD\")\n",
    "        print(f\"  Profit Margin: {(df['net_profit'].sum() / df['total_revenue'].sum() * 100):.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loading-header",
   "metadata": {},
   "source": [
    "## 3. Database Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "create-database",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "CREATING DATA WAREHOUSE\n",
      "======================================================================\n",
      "======================================================================\n",
      "TECHSTORE DATA WAREHOUSE - DATABASE LOADING\n",
      "Star Schema: 1 Fact Table + 4 Dimension Tables\n",
      "======================================================================\n",
      "\n",
      "[1/4] Loading transformed Star Schema files...\n",
      "  Dim_Customer: 1,110 rows loaded\n",
      "  Dim_Date: 731 rows loaded\n",
      "  Dim_Product: 38 rows loaded\n",
      "  Dim_Store: 12 rows loaded\n",
      "  Fact_Sales: 25,000 rows loaded\n",
      "\n",
      "[2/4] Standardizing column names for database...\n",
      "\n",
      "  [DEBUG] Available columns in each dataframe:\n",
      "    Dim_Customer: ['customer_id', 'full_name', 'city_id', 'city_name', 'region']\n",
      "    Dim_Product: ['product_id', 'product_name', 'subcategory_id', 'subcategory_name', 'category_id', 'category_name', 'unit_price', 'unit_cost', 'competitor_price', 'price_difference', 'price_difference_pct', 'avg_sentiment', 'avg_rating', 'review_count']\n",
      "    Dim_Store: ['store_id', 'store_name', 'city_id', 'city_name', 'region', 'monthly_target', 'annual_target', 'manager_name']\n",
      "  Column mapping completed\n",
      "\n",
      "  Final column counts:\n",
      "    Dim_Customer: 4 columns\n",
      "    Dim_Product: 7 columns\n",
      "    Dim_Store: 7 columns\n",
      "    Dim_Date: 10 columns\n",
      "    Fact_Sales: 11 columns\n",
      "\n",
      "[3/4] Creating SQLite Data Warehouse...\n",
      "  Old database deleted\n",
      "  Connected to: c:\\Users\\windows\\TechStore\\src\\database\\techstore_dw.db\n",
      "\n",
      "  Creating Star Schema tables...\n",
      "    Dim_Date created\n",
      "    Dim_Product created\n",
      "    Dim_Store created\n",
      "    Dim_Customer created\n",
      "    Fact_Sales created\n",
      "\n",
      "[4/4] Loading data into tables...\n",
      "  Loading dimensions first (for referential integrity)...\n",
      "    Dim_Date: 731 rows inserted\n",
      "    Dim_Product: 38 rows inserted\n",
      "    Dim_Store: 12 rows inserted\n",
      "    Dim_Customer: 1,110 rows inserted\n",
      "  Loading fact table...\n",
      "    Fact_Sales: 25,000 rows inserted\n",
      "\n",
      "  All data loaded successfully!\n",
      "\n",
      "[Verification] Checking database integrity...\n",
      "\n",
      "  Table Row Counts:\n",
      "  ------------------------------------------------------------\n",
      "    Dim_Date                    731 rows\n",
      "    Dim_Product                  38 rows\n",
      "    Dim_Store                    12 rows\n",
      "    Dim_Customer              1,110 rows\n",
      "    Fact_Sales               25,000 rows\n",
      "  ------------------------------------------------------------\n",
      "\n",
      "  Testing Star Schema joins (Top 5 Sales by Revenue)...\n",
      "  ------------------------------------------------------------\n",
      " Sale_ID  Full_Date Product_Name              Store_Name    Customer_Name  Quantity  Revenue    Profit\n",
      "     248 2023-09-16  DJI Mavic 3 TechStore Tlemcen Ville    Meriem Benali         1 360000.0 139684.52\n",
      "     250 2024-12-28  DJI Mavic 3  TechStore Alger Centre Meriem Bouchareb         1 360000.0 137919.34\n",
      "     331 2023-03-09  DJI Mavic 3 TechStore Tlemcen Ville    Hichem Amrani         1 360000.0 138357.14\n",
      "     606 2023-08-13  DJI Mavic 3  TechStore Alger Centre      Sami Benali         1 360000.0 137830.82\n",
      "     756 2023-06-17  DJI Mavic 3    TechStore Setif Park   Fatima Guellil         1 360000.0 139599.78\n",
      "\n",
      "  Star Schema joins working correctly!\n",
      "\n",
      "  Business Metrics Summary:\n",
      "  ------------------------------------------------------------\n",
      " Unique_Products  Unique_Stores  Unique_Customers  Total_Transactions  Total_Revenue_M_DZD  Total_Profit_M_DZD  Profit_Margin_Pct  Avg_Transaction_Value\n",
      "              38             12              1200               25000               859.66              259.49              30.19               34386.47\n",
      "\n",
      "  Date Range: 2023-01-01 to 2024-12-31\n",
      "\n",
      "======================================================================\n",
      "DATA WAREHOUSE LOADING COMPLETED SUCCESSFULLY!\n",
      "======================================================================\n",
      "\n",
      "Database File: c:\\Users\\windows\\TechStore\\src\\database\\techstore_dw.db\n",
      "Date Range: 2023-01-01 to 2024-12-31\n",
      "Total Revenue: 859.66 Million DZD\n",
      "Total Profit: 259.49 Million DZD\n",
      "Profit Margin: 30.19%\n",
      "Stores: 12\n",
      "Products: 38\n",
      "Customers: 1200\n",
      "Transactions: 25,000\n",
      "\n",
      "======================================================================\n",
      "STAR SCHEMA TABLES (5)\n",
      "STAR SCHEMA TABLES (5)\n",
      "======================================================================\n",
      "1. Fact_Sales (Fact Table)\n",
      "2. Dim_Date (Time Dimension)\n",
      "3. Dim_Product (Product Dimension)\n",
      "4. Dim_Store (Store Dimension)\n",
      "5. Dim_Customer (Customer Dimension)\n",
      "1. Fact_Sales (Fact Table)\n",
      "2. Dim_Date (Time Dimension)\n",
      "3. Dim_Product (Product Dimension)\n",
      "4. Dim_Store (Store Dimension)\n",
      "5. Dim_Customer (Customer Dimension)\n",
      "======================================================================\n",
      "\n",
      "Data warehouse created successfully\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CREATING DATA WAREHOUSE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "try:\n",
    "    os.chdir(SCRIPTS_DIR)\n",
    "    exec(open('create_database.py').read())\n",
    "    os.chdir(PROJECT_ROOT)\n",
    "    print(\"\\nData warehouse created successfully\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Database creation error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    os.chdir(PROJECT_ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "test-connection",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "DATABASE CONNECTION TEST\n",
      "======================================================================\n",
      "\n",
      "Connected: c:\\Users\\windows\\TechStore\\src\\database\\techstore_dw.db\n",
      "\n",
      "Tables (5):\n",
      "  Dim_Customer            1,110 rows\n",
      "  Dim_Date                  731 rows\n",
      "  Dim_Product                38 rows\n",
      "  Dim_Store                  12 rows\n",
      "  Fact_Sales             25,000 rows\n",
      "\n",
      "Testing Star Schema joins:\n",
      " Sale_ID  Full_Date Product_Name                  Store_Name  Customer_Name  Total_Revenue  Net_Profit\n",
      "       1 2024-07-02 Redmi Buds 4 TechStore Constantine Cirta    Ryad Benali         9000.0     1956.37\n",
      "       2 2023-07-01 HP 650 Black       TechStore Ouargla Sud Mohamed Benali         2500.0      417.16\n",
      "       4 2024-08-01 Redmi Buds 4        TechStore Oran Bahia Sarah Zerrouki         4500.0      364.51\n",
      "       5 2023-04-30  Oppo Reno 8      TechStore Biskra Ziban   Amel Brahami        58000.0    19474.00\n",
      "       6 2024-06-25 HP 650 Black       TechStore Ouargla Sud  Fares Khelifa         2500.0      437.77\n",
      "\n",
      "Joins working correctly\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DATABASE CONNECTION TEST\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "try:\n",
    "    import sqlite3\n",
    "    \n",
    "    db_path = DATABASE_DIR / 'techstore_dw.db'\n",
    "    \n",
    "    if db_path.exists():\n",
    "        conn = sqlite3.connect(str(db_path))\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table' ORDER BY name\")\n",
    "        tables = cursor.fetchall()\n",
    "        \n",
    "        print(f\"\\nConnected: {db_path}\")\n",
    "        print(f\"\\nTables ({len(tables)}):\")\n",
    "        \n",
    "        for table in tables:\n",
    "            table_name = table[0]\n",
    "            cursor.execute(f\"SELECT COUNT(*) FROM {table_name}\")\n",
    "            count = cursor.fetchone()[0]\n",
    "            print(f\"  {table_name:20} {count:>8,} rows\")\n",
    "        \n",
    "        print(\"\\nTesting Star Schema joins:\")\n",
    "        test_query = \"\"\"\n",
    "        SELECT \n",
    "            fs.Sale_ID,\n",
    "            dd.Full_Date,\n",
    "            dp.Product_Name,\n",
    "            ds.Store_Name,\n",
    "            dc.Customer_Name,\n",
    "            fs.Total_Revenue,\n",
    "            fs.Net_Profit\n",
    "        FROM Fact_Sales fs\n",
    "        JOIN Dim_Date dd ON fs.Date_ID = dd.Date_ID\n",
    "        JOIN Dim_Product dp ON fs.Product_ID = dp.Product_ID\n",
    "        JOIN Dim_Store ds ON fs.Store_ID = ds.Store_ID\n",
    "        JOIN Dim_Customer dc ON fs.Customer_ID = dc.Customer_ID\n",
    "        LIMIT 5\n",
    "        \"\"\"\n",
    "        \n",
    "        test_df = pd.read_sql(test_query, conn)\n",
    "        print(test_df.to_string(index=False))\n",
    "        print(\"\\nJoins working correctly\")\n",
    "        \n",
    "        conn.close()\n",
    "    else:\n",
    "        print(f\"Database not found: {db_path}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Database test error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "validation-header",
   "metadata": {},
   "source": [
    "## 4. Validation & Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "data-quality",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "DATA QUALITY VALIDATION\n",
      "======================================================================\n",
      "\n",
      "       Table  Rows  Columns  Missing_Values Missing_Pct  Duplicate_Keys Status\n",
      "Dim_Customer  1110        5               0       0.00%               0     OK\n",
      " Dim_Product    38       14               0       0.00%               0     OK\n",
      "   Dim_Store    12        8               0       0.00%               0     OK\n",
      "    Dim_Date   731       10               0       0.00%               0     OK\n",
      "  Fact_Sales 25000       12               0       0.00%               0     OK\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DATA QUALITY VALIDATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "validation_results = []\n",
    "\n",
    "for name, df in transformed_data.items():\n",
    "    missing = df.isnull().sum().sum()\n",
    "    missing_pct = (missing / (df.shape[0] * df.shape[1])) * 100\n",
    "    \n",
    "    if 'Customer_ID' in df.columns:\n",
    "        duplicates = df['Customer_ID'].duplicated().sum()\n",
    "    elif 'Product_ID' in df.columns:\n",
    "        duplicates = df['Product_ID'].duplicated().sum()\n",
    "    elif 'Store_ID' in df.columns:\n",
    "        duplicates = df['Store_ID'].duplicated().sum()\n",
    "    elif 'Date_ID' in df.columns:\n",
    "        duplicates = df['Date_ID'].duplicated().sum()\n",
    "    elif 'Sale_ID' in df.columns:\n",
    "        duplicates = df['Sale_ID'].duplicated().sum()\n",
    "    else:\n",
    "        duplicates = 0\n",
    "    \n",
    "    validation_results.append({\n",
    "        'Table': name,\n",
    "        'Rows': len(df),\n",
    "        'Columns': len(df.columns),\n",
    "        'Missing_Values': missing,\n",
    "        'Missing_Pct': f\"{missing_pct:.2f}%\",\n",
    "        'Duplicate_Keys': duplicates,\n",
    "        'Status': 'OK' if missing_pct < 5 and duplicates == 0 else 'WARNING'\n",
    "    })\n",
    "\n",
    "df_validation = pd.DataFrame(validation_results)\n",
    "print(\"\\n\" + df_validation.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "sql-tests",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "SQL QUERIES TEST\n",
      "======================================================================\n",
      "======================================================================\n",
      "COMPREHENSIVE SQL QUERIES TEST\n",
      "======================================================================\n",
      "Connected to: ../database/techstore_dw.db\n",
      "\n",
      "[1/13] Testing: Total Revenue\n",
      "  Success: 1 rows, 1 columns\n",
      "  Result:\n",
      " Total_Revenue\n",
      "   859661700.0\n",
      "\n",
      "[2/13] Testing: Net Profit\n",
      "  Success: 1 rows, 1 columns\n",
      "  Result:\n",
      "  Net_Profit\n",
      "259493425.49\n",
      "\n",
      "[3/13] Testing: Total Transactions\n",
      "  Success: 1 rows, 1 columns\n",
      "  Result:\n",
      " Total_Transactions\n",
      "              25000\n",
      "\n",
      "[4/13] Testing: Avg Transaction Value\n",
      "  Success: 1 rows, 1 columns\n",
      "  Result:\n",
      " Avg_Transaction_Value\n",
      "              34386.47\n",
      "\n",
      "[5/13] Testing: Monthly Trends\n",
      "  Success: 24 rows, 7 columns\n",
      "\n",
      "[6/13] Testing: Top Selling Products\n",
      "  Success: 15 rows, 6 columns\n",
      "\n",
      "[7/13] Testing: Category Performance\n",
      "  Success: 5 rows, 6 columns\n",
      "\n",
      "[8/13] Testing: Store Ranking\n",
      "  Success: 12 rows, 8 columns\n",
      "\n",
      "[9/13] Testing: Regional Performance\n",
      "  Success: 4 rows, 6 columns\n",
      "\n",
      "[10/13] Testing: Top Customers\n",
      "  Success: 20 rows, 7 columns\n",
      "\n",
      "[11/13] Testing: Profit Margin by Category\n",
      "  Success: 5 rows, 9 columns\n",
      "\n",
      "[12/13] Testing: Marketing ROI\n",
      "  Success: 5 rows, 5 columns\n",
      "\n",
      "[13/13] Testing: Dashboard Summary\n",
      "  Success: 1 rows, 12 columns\n",
      "  Result:\n",
      "Start_Date   End_Date  Total_Transactions  Unique_Customers  Unique_Products  Unique_Stores  Total_Revenue  Total_Profit  Avg_Transaction_Value  Overall_Profit_Margin  Total_Units_Sold  Avg_Quantity_Per_Transaction\n",
      "2023-01-01 2024-12-31               16652              1200               38             12    570222100.0  172099726.93               34243.46                  30.18             27444                          1.65\n",
      "\n",
      "======================================================================\n",
      "TEST SUMMARY\n",
      "======================================================================\n",
      "                    Query Status  Rows  Columns\n",
      "            Total Revenue   Pass     1        1\n",
      "               Net Profit   Pass     1        1\n",
      "       Total Transactions   Pass     1        1\n",
      "    Avg Transaction Value   Pass     1        1\n",
      "           Monthly Trends   Pass    24        7\n",
      "     Top Selling Products   Pass    15        6\n",
      "     Category Performance   Pass     5        6\n",
      "            Store Ranking   Pass    12        8\n",
      "     Regional Performance   Pass     4        6\n",
      "            Top Customers   Pass    20        7\n",
      "Profit Margin by Category   Pass     5        9\n",
      "            Marketing ROI   Pass     5        5\n",
      "        Dashboard Summary   Pass     1       12\n",
      "\n",
      "Passed: 13\n",
      "Failed: 0\n",
      "\n",
      "All queries are working perfectly!\n",
      "Ready for dashboard integration!\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SQL QUERIES TEST\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "try:\n",
    "    os.chdir(SCRIPTS_DIR)\n",
    "    exec(open('test_queries.py').read())\n",
    "    os.chdir(PROJECT_ROOT)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"SQL tests error: {e}\")\n",
    "    os.chdir(PROJECT_ROOT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary-header",
   "metadata": {},
   "source": [
    "## 5. Pipeline Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "final-summary",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "ETL PIPELINE COMPLETED\n",
      "======================================================================\n",
      "\n",
      "EXTRACTION:\n",
      "  MySQL Tables: 8 [OK]\n",
      "  Competitor Prices: True [OK]\n",
      "  Legacy Invoices: True [OK]\n",
      "\n",
      "TRANSFORMATION:\n",
      "  Dimensions: 4 [OK]\n",
      "  Fact Table: True [OK]\n",
      "\n",
      "LOADING:\n",
      "  Database: True [OK]\n",
      "  Tables: 5 [OK]\n",
      "\n",
      "NEXT STEPS:\n",
      "  1. Launch dashboard: streamlit run dashboard/dashboard_app.py\n",
      "  2. Analyze business metrics\n",
      "  3. Generate final report\n",
      "\n",
      "======================================================================\n",
      "FILES GENERATED:\n",
      "======================================================================\n",
      "  Data Warehouse: c:\\Users\\windows\\TechStore\\src\\database\\techstore_dw.db\n",
      "  Extracted Data: c:\\Users\\windows\\TechStore\\src\\Data\\extracted/\n",
      "  Transformed Data: c:\\Users\\windows\\TechStore\\src\\Data\\transformed/\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ETL PIPELINE COMPLETED\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "summary = {\n",
    "    'Extraction': {\n",
    "        'MySQL Tables': len([k for k in dataframes.keys() if k not in ['competitor_prices']]),\n",
    "        'Competitor Prices': 'competitor_prices' in dataframes,\n",
    "        'Legacy Invoices': (DATA_DIR / 'extracted' / 'legacy_sales.csv').exists()\n",
    "    },\n",
    "    'Transformation': {\n",
    "        'Dimensions': len([k for k in transformed_data.keys() if k.startswith('Dim_')]),\n",
    "        'Fact Table': 'Fact_Sales' in transformed_data\n",
    "    },\n",
    "    'Loading': {\n",
    "        'Database': (DATABASE_DIR / 'techstore_dw.db').exists(),\n",
    "        'Tables': len(tables) if 'tables' in locals() else 0\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"\\nEXTRACTION:\")\n",
    "for key, value in summary['Extraction'].items():\n",
    "    status = \"OK\" if value else \"SKIPPED\"\n",
    "    print(f\"  {key}: {value} [{status}]\")\n",
    "\n",
    "print(\"\\nTRANSFORMATION:\")\n",
    "for key, value in summary['Transformation'].items():\n",
    "    status = \"OK\" if value else \"FAILED\"\n",
    "    print(f\"  {key}: {value} [{status}]\")\n",
    "\n",
    "print(\"\\nLOADING:\")\n",
    "for key, value in summary['Loading'].items():\n",
    "    status = \"OK\" if value else \"FAILED\"\n",
    "    print(f\"  {key}: {value} [{status}]\")\n",
    "\n",
    "print(\"\\nNEXT STEPS:\")\n",
    "print(\"  1. Launch dashboard: streamlit run dashboard/dashboard_app.py\")\n",
    "print(\"  2. Analyze business metrics\")\n",
    "print(\"  3. Generate final report\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FILES GENERATED:\")\n",
    "print(\"=\"*70)\n",
    "print(f\"  Data Warehouse: {DATABASE_DIR / 'techstore_dw.db'}\")\n",
    "print(f\"  Extracted Data: {EXTRACTED_DIR}/\")\n",
    "print(f\"  Transformed Data: {TRANSFORMED_DIR}/\")\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
