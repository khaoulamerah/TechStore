{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da295553",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================\n",
    "# PROJET BUSINESS INTELLIGENCE - TECHSTORE\n",
    "# ETL PIPELINE - PARTIE MEMBRE 1 : DATA EXTRACTION\n",
    "# ====================================================================\n",
    "\n",
    "# %% [markdown]\n",
    "# # üìä Extraction des Donn√©es (Membre 1)\n",
    "# \n",
    "# Ce notebook contient la partie extraction du pipeline ETL.\n",
    "# **Responsable :** Membre 1 - Data Extraction Engineer\n",
    "# \n",
    "# ## Objectifs :\n",
    "# 1. ‚úÖ Extraire les donn√©es de MySQL (ERP)\n",
    "# 2. ‚úÖ Scraper les prix des concurrents\n",
    "# # 3. ‚úÖ Valider la qualit√© des donn√©es extraites\n",
    "\n",
    "# %% Imports pour tout le pipeline ETL\n",
    "# Installer les d√©pendances manquantes (ex√©cuter uniquement dans le notebook)\n",
    "\n",
    "\n",
    "# === Imports de base ===\n",
    "import pandas as pd\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# === Partie 1 : Extraction (Membre 1) ===\n",
    "import mysql.connector\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "\n",
    "# === Partie 2 : Transformations (Membre 2 - Toi) ===\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import pytesseract\n",
    "from PIL import Image\n",
    "import re\n",
    "from fuzzywuzzy import process  # Pour le matching des prix concurrents\n",
    "\n",
    "print(\"‚úÖ Biblioth√®ques import√©es avec succ√®s\")\n",
    "print(f\"üìÖ Date d'ex√©cution: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 1Ô∏è‚É£ Configuration de la Connexion MySQL\n",
    "\n",
    "# %% Configuration\n",
    "MYSQL_CONFIG = {\n",
    "    'host': 'boughida.com',\n",
    "    'database': 'techstore_erp',\n",
    "    'user': 'student_user_4ing',\n",
    "    'password': 'bi_guelma_2025'\n",
    "}\n",
    "\n",
    "# Cr√©er les r√©pertoires n√©cessaires\n",
    "os.makedirs('data/extracted', exist_ok=True)\n",
    "\n",
    "print(\"üìã Configuration charg√©e\")\n",
    "print(f\"   Serveur: {MYSQL_CONFIG['host']}\")\n",
    "print(f\"   Base de donn√©es: {MYSQL_CONFIG['database']}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 2Ô∏è‚É£ Connexion et Test\n",
    "\n",
    "# %% Test de connexion\n",
    "def test_connection():\n",
    "    \"\"\"Tester la connexion √† MySQL\"\"\"\n",
    "    try:\n",
    "        conn = mysql.connector.connect(**MYSQL_CONFIG)\n",
    "        if conn.is_connected():\n",
    "            print(\"‚úÖ Connexion MySQL r√©ussie!\")\n",
    "            \n",
    "            # Tester une requ√™te simple\n",
    "            cursor = conn.cursor()\n",
    "            cursor.execute(\"SELECT DATABASE()\")\n",
    "            db_name = cursor.fetchone()[0]\n",
    "            print(f\"   Base de donn√©es active: {db_name}\")\n",
    "            \n",
    "            # Lister les tables disponibles\n",
    "            cursor.execute(\"SHOW TABLES\")\n",
    "            tables = cursor.fetchall()\n",
    "            print(f\"   Nombre de tables: {len(tables)}\")\n",
    "            \n",
    "            conn.close()\n",
    "            return True\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erreur de connexion: {e}\")\n",
    "        return False\n",
    "\n",
    "# Ex√©cuter le test\n",
    "test_connection()\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 3Ô∏è‚É£ Extraction des Tables MySQL\n",
    "\n",
    "# %% Fonction d'extraction\n",
    "def extract_mysql_table(table_name, conn):\n",
    "    \"\"\"Extraire une table MySQL vers DataFrame\"\"\"\n",
    "    try:\n",
    "        print(f\"üìä Extraction: {table_name}...\", end=\" \")\n",
    "        \n",
    "        query = f\"SELECT * FROM {table_name}\"\n",
    "        df = pd.read_sql(query, conn)\n",
    "        \n",
    "        # Sauvegarder en CSV\n",
    "        filename = f\"data/extracted/{table_name.replace('table_', '')}.csv\"\n",
    "        df.to_csv(filename, index=False, encoding='utf-8')\n",
    "        \n",
    "        print(f\"‚úÖ {len(df)} lignes | {len(df.columns)} colonnes\")\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erreur: {e}\")\n",
    "        return None\n",
    "\n",
    "# %% Extraction de toutes les tables\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üöÄ EXTRACTION DES TABLES MySQL\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "# Se connecter\n",
    "conn = mysql.connector.connect(**MYSQL_CONFIG)\n",
    "\n",
    "# Liste des tables √† extraire\n",
    "tables_to_extract = [\n",
    "    'table_sales',\n",
    "    'table_products',\n",
    "    'table_reviews',\n",
    "    'table_customers',\n",
    "    'table_stores',\n",
    "    'table_cities',\n",
    "    'table_categories',\n",
    "    'table_subcategories'\n",
    "]\n",
    "\n",
    "# Dictionnaire pour stocker les DataFrames\n",
    "dataframes = {}\n",
    "\n",
    "# Extraire chaque table\n",
    "for table in tables_to_extract:\n",
    "    df = extract_mysql_table(table, conn)\n",
    "    if df is not None:\n",
    "        # Enlever le pr√©fixe \"table_\" pour le nom\n",
    "        clean_name = table.replace('table_', '')\n",
    "        dataframes[clean_name] = df\n",
    "\n",
    "# Fermer la connexion\n",
    "conn.close()\n",
    "\n",
    "print(\"\\n‚úÖ Extraction MySQL termin√©e!\")\n",
    "print(f\"üì¶ {len(dataframes)} tables extraites\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 4Ô∏è‚É£ Aper√ßu des Donn√©es Extraites\n",
    "\n",
    "# %% Afficher un aper√ßu\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìã APER√áU DES DONN√âES EXTRAITES\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "for name, df in dataframes.items():\n",
    "    print(f\"\\nüìä Table: {name}\")\n",
    "    print(f\"   Dimensions: {df.shape[0]} lignes √ó {df.shape[1]} colonnes\")\n",
    "    print(f\"   Colonnes: {', '.join(df.columns.tolist()[:5])}...\")\n",
    "    print(f\"   Aper√ßu:\")\n",
    "    display(df.head(3))\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 5Ô∏è‚É£ Statistiques des Ventes\n",
    "\n",
    "# %% Analyse rapide des ventes\n",
    "df_sales = dataframes['sales']\n",
    "\n",
    "print(\"\\nüìä STATISTIQUES DES VENTES\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Nombre total de ventes: {len(df_sales):,}\")\n",
    "print(f\"Revenu total: {df_sales['Total_Revenue'].sum():,.2f} DZD\")\n",
    "print(f\"Revenu moyen par vente: {df_sales['Total_Revenue'].mean():,.2f} DZD\")\n",
    "print(f\"P√©riode: {df_sales['Date'].min()} ‚Üí {df_sales['Date'].max()}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 6Ô∏è‚É£ Web Scraping - Prix Concurrents\n",
    "\n",
    "# %% Configuration du scraping\n",
    "COMPETITOR_URL = \"https://boughida.com/competitor/\"\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üï∑Ô∏è  WEB SCRAPING - PRIX CONCURRENTS\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "# %% Fonction de scraping\n",
    "def scrape_competitor_prices(url):\n",
    "    \"\"\"Scraper les prix des concurrents\"\"\"\n",
    "    try:\n",
    "        print(f\"üì° Connexion √†: {url}\")\n",
    "        \n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "        }\n",
    "        \n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        products = []\n",
    "        \n",
    "        # Chercher les produits (adapter selon la structure HTML)\n",
    "        product_items = soup.find_all('div', class_='product-item')\n",
    "        \n",
    "        print(f\"üîç {len(product_items)} produits trouv√©s\")\n",
    "        \n",
    "        for item in product_items:\n",
    "            try:\n",
    "                # Extraire le nom\n",
    "                name_elem = item.find('h3')\n",
    "                name = name_elem.text.strip() if name_elem else None\n",
    "                \n",
    "                # Extraire le prix\n",
    "                price_elem = item.find('span', class_='price')\n",
    "                if price_elem:\n",
    "                    price_text = price_elem.text.strip()\n",
    "                    # Nettoyer le prix\n",
    "                    import re\n",
    "                    price = float(re.sub(r'[^\\\\d.]', '', price_text))\n",
    "                else:\n",
    "                    price = None\n",
    "                \n",
    "                if name and price:\n",
    "                    products.append({\n",
    "                        'Competitor_Product_Name': name,\n",
    "                        'Competitor_Price': price\n",
    "                    })\n",
    "                    \n",
    "            except Exception as e:\n",
    "                continue\n",
    "        \n",
    "        return pd.DataFrame(products)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erreur de scraping: {e}\")\n",
    "        print(\"üìù Utilisation de donn√©es de test √† la place\")\n",
    "        return create_test_competitor_data()\n",
    "\n",
    "# %% Fonction de donn√©es de test\n",
    "def create_test_competitor_data():\n",
    "    \"\"\"Cr√©er des donn√©es de test pour les prix concurrents\"\"\"\n",
    "    test_data = [\n",
    "        {'Competitor_Product_Name': 'Laptop HP ProBook 450', 'Competitor_Price': 95000},\n",
    "        {'Competitor_Product_Name': 'Dell Latitude 5420', 'Competitor_Price': 105000},\n",
    "        {'Competitor_Product_Name': 'iPhone 14 128GB', 'Competitor_Price': 165000},\n",
    "        {'Competitor_Product_Name': 'Samsung S23 Ultra', 'Competitor_Price': 185000},\n",
    "        {'Competitor_Product_Name': 'Sony WH-1000XM5', 'Competitor_Price': 42000},\n",
    "        {'Competitor_Product_Name': 'AirPods Pro 2', 'Competitor_Price': 35000},\n",
    "        {'Competitor_Product_Name': 'LG OLED55C3', 'Competitor_Price': 215000},\n",
    "        {'Competitor_Product_Name': 'Samsung QN65Q80C', 'Competitor_Price': 275000},\n",
    "    ]\n",
    "    return pd.DataFrame(test_data)\n",
    "\n",
    "# %% Ex√©cuter le scraping\n",
    "df_competitor = scrape_competitor_prices(COMPETITOR_URL)\n",
    "\n",
    "# Sauvegarder\n",
    "df_competitor.to_csv('data/extracted/competitor_prices.csv', index=False)\n",
    "\n",
    "print(f\"\\n‚úÖ Scraping termin√©: {len(df_competitor)} produits extraits\")\n",
    "print(f\"üíæ Fichier sauvegard√©: data/extracted/competitor_prices.csv\")\n",
    "\n",
    "# Afficher un aper√ßu\n",
    "print(\"\\nüìã Aper√ßu des prix concurrents:\")\n",
    "display(df_competitor.head(10))\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 7Ô∏è‚É£ Validation des Donn√©es Extraites\n",
    "\n",
    "# %% V√©rifications de qualit√©\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ VALIDATION DES DONN√âES\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "validation_results = []\n",
    "\n",
    "for name, df in dataframes.items():\n",
    "    # V√©rifier les valeurs manquantes\n",
    "    missing = df.isnull().sum().sum()\n",
    "    missing_pct = (missing / (df.shape[0] * df.shape[1])) * 100\n",
    "    \n",
    "    # V√©rifier les doublons\n",
    "    duplicates = df.duplicated().sum()\n",
    "    \n",
    "    validation_results.append({\n",
    "        'Table': name,\n",
    "        'Lignes': len(df),\n",
    "        'Colonnes': len(df.columns),\n",
    "        'Valeurs_Manquantes': missing,\n",
    "        'Pct_Manquant': f\"{missing_pct:.2f}%\",\n",
    "        'Doublons': duplicates,\n",
    "        'Statut': '‚úÖ' if missing_pct < 5 and duplicates < 10 else '‚ö†Ô∏è'\n",
    "    })\n",
    "\n",
    "df_validation = pd.DataFrame(validation_results)\n",
    "display(df_validation)\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 8Ô∏è‚É£ R√©sum√© de l'Extraction\n",
    "\n",
    "# %% R√©sum√© final\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìä R√âSUM√â DE L'EXTRACTION (MEMBRE 1)\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "print(\"‚úÖ T√ÇCHES COMPL√âT√âES:\")\n",
    "print(\"   1. Connexion MySQL √©tablie et test√©e\")\n",
    "print(\"   2. 8 tables extraites de l'ERP\")\n",
    "print(\"   3. Web scraping des prix concurrents effectu√©\")\n",
    "print(\"   4. Toutes les donn√©es sauvegard√©es en CSV\")\n",
    "print(\"   5. Validation de la qualit√© des donn√©es effectu√©e\")\n",
    "\n",
    "print(\"\\nüì¶ FICHIERS CR√â√âS:\")\n",
    "for file in os.listdir('data/extracted'):\n",
    "    file_path = f\"data/extracted/{file}\"\n",
    "    file_size = os.path.getsize(file_path) / 1024  # KB\n",
    "    print(f\"   ‚Ä¢ {file} ({file_size:.1f} KB)\")\n",
    "\n",
    "print(\"\\nüöÄ PROCHAINE √âTAPE:\")\n",
    "print(\"   ‚Üí Membre 2 peut maintenant transformer ces donn√©es\")\n",
    "print(\"   ‚Üí Fichiers disponibles dans: data/extracted/\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚úÖ EXTRACTION TERMIN√âE AVEC SUCC√àS\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.14.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
